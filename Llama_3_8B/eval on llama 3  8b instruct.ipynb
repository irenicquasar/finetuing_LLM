{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running eval on gsm8k on llama 3 8b"
      ],
      "metadata": {
        "id": "lveRIi77q7oD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI-8lchcqxPS"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cli login\n",
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "OrQ6ajHbrfx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Define the model name\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Ensure the pad token is set correctly\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Function to generate response\n",
        "def generate_response(prompt, max_length=512):\n",
        "    # Format the input as per the model's expected chat format\n",
        "    formatted_prompt = f\"[USER]: {prompt}\\n[ASSISTANT]:\"\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        formatted_prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            temperature=0.7,\n",
        "            num_return_sequences=1\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Extract only the assistant's response\n",
        "    assistant_response = response.split(\"[ASSISTANT]:\")[1].strip()\n",
        "    return assistant_response\n",
        "\n",
        "# Example usage\n",
        "test_prompt = \"Explain the Pythagorean theorem\"\n",
        "response = generate_response(test_prompt)\n",
        "print(f\"Prompt: {test_prompt}\")\n",
        "print(f\"Response: {response}\")\n"
      ],
      "metadata": {
        "id": "cCvX-ymqrIyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#eval\n",
        "!pip install lm_eval"
      ],
      "metadata": {
        "id": "XuqBHr2yrNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lm_eval\n",
        "model_eval = lm_eval.models.huggingface.HFLM(pretrained=model, tokenizer=tokenizer)\n",
        "result  = lm_eval.evaluator.simple_evaluate(model_eval, tasks=[\"gsm8k\"], device = 'gpu',num_fewshot=0, batch_size=8)['results']\n",
        "print(result)"
      ],
      "metadata": {
        "id": "8NXYN1RArRPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}